{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PyTorch Transformer Model for Classification.ipynb","provenance":[],"authorship_tag":"ABX9TyN+LUvoPacRN04MrVggUccK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# PyTorch Transformer Model for Classification: Input-Output\n","[Site: PyTorch Transformer Model for Classification: Input-Output](https://jamesmccaffrey.wordpress.com/2021/03/02/pytorch-transformer-model-for-classification-input-output/)"],"metadata":{"id":"fwh9LQx3186L"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"GBRn_EPNzjEo","executionInfo":{"status":"ok","timestamp":1649767536205,"user_tz":-540,"elapsed":7614,"user":{"displayName":"harasho amplil","userId":"06889073562372060382"}}},"outputs":[],"source":["# imdb_transformer_io.py\n","# IMDB classification, loosely  based on the PyTorch docs\n","\n","# Input-Ouput only. Work in progress. Surely has bugs.\n","\n","# Python 3.7.6, PyTorch 1.7.0\n","# Windows 10\n","# uses BucketIterator - so results not reproducible.\n","# data has been manually pruned from 25,000 train and\n","# 25,000 test to 200 train and 200 test\n","\n","import torch as T\n","import torchtext as tt\n","import numpy as np\n","import math\n","import warnings  # most of torchtext is deprecated\n","warnings.filterwarnings(\"ignore\")\n","\n","device = T.device(\"cpu\")"]},{"cell_type":"code","source":["class TransformerModel(T.nn.Module):\n","  # Transformer model for IMDB binary classification\n","  # n_tokens: num distinct words == vocabulary size\n","  # embed_dim: num values for each word/token is embed_dim\n","  # n_heads: num attention heads, needed by Tr_EncoderLayer\n","  # n_hid : num hidden nodes in NN part of Tr_EncoderLayer\n","  # n_eclayers: num Tr_EncoderLayer layers in Tr_Encoder\n","  # drop_p is used by PositionalEncoding AND Tr_EncoderLayer\n","\n","  def __init__(self, n_tokens, embed_dim, n_heads, n_hid, \\\n","    n_eclayers, drop_p=0.5):\n","    super(TransformerModel, self).__init__()\n","    self.embed_dim = embed_dim\n","\n","    self.embedder = T.nn.Embedding(n_tokens, embed_dim)\n","    self.pos_encoder = PositionalEncoding(embed_dim, drop_p)\n","    enc_layer = T.nn.TransformerEncoderLayer(embed_dim, \\\n","      n_heads, n_hid, drop_p)\n","    self.transformer_encoder = \\\n","      T.nn.TransformerEncoder(enc_layer, n_eclayers)\n","    # map 4 embed vals to 2 classes\n","    self.to_logits = T.nn.Linear(embed_dim, 2)  \n","\n","    self.embedder.weight.data.uniform_(-0.01, 0.01)\n","    self.to_logits.weight.data.uniform_(-0.01, 0.01)\n","    self.to_logits.bias.data.zero_()\n","\n","  def forward(self, src, src_mask):\n","    # assumes src is [seq, bat] -- \n","    # but most default data loaders\n","    # give [bat, seq] and so must \n","    # 1.) reshape src HERE, or\n","    # 2.) reshape src outside, before feeding to forward(), or\n","    # 3.) modify loader by using batch_first=False if available\n","    # note: no mask needed for classification problems\n","\n","    print(\"src entering forward, before encoder * sqrt:\")\n","    print(src); print(src.shape); input()  # [5,3], [seq,bat]\n","\n","    z = self.embedder(src) * math.sqrt(self.embed_dim) \n","\n","    print(\"z after embedding, before pos_encoder \")\n","    print(z); print(z.shape); input()  # [5,3,4], [seq,bat,emb]\n","\n","    z = self.pos_encoder(z)\n","\n","    print(\"z after pos_encoder, before trans_encoder \")\n","    print(z); print(z.shape); input()  # [5,3,4]\n","\n","    oupt = self.transformer_encoder(z)\n","\n","    print(\"prelim output from trans_encoder, before pooling:\")\n","    print(oupt); print(oupt.shape); input()\n","    # [5,3,4] same shape as embedded input: seq-to-seq  \n","\n","    # combine output sequences to 'one word' of review\n","    oupt = oupt.max(dim=0)[0]  # [3,4] == [bat, 'one word']\n","    # avg pooling is one of many promising alternatives\n","    # oupt = oupt.mean(dim=0)      # [3,4]\n","\n","    print(\"prelim oupt after max pooling, before to_logits\")\n","    print(oupt); print(oupt.shape); input()\n","\n","    # map 'one word' to 2 predictions (as logits)\n","    oupt = self.to_logits(oupt)    # [3,2] == [bat, class]\n","\n","    print(\"prelim oupt after to_logits, before log_softmax:\")\n","    print(oupt); print(oupt.shape); input()\n","\n","    # apply log_softmax so oupt can be used by loss_func\n","    result = T.nn.functional.log_softmax(oupt, dim=1)\n","\n","    print(\"final result after log_softmax, \\\n","      before return statement:\")\n","    print(result); print(result.shape); input()\n","\n","    return result"],"metadata":{"id":"l3Px7IFLznzL","executionInfo":{"status":"ok","timestamp":1649767536206,"user_tz":-540,"elapsed":6,"user":{"displayName":"harasho amplil","userId":"06889073562372060382"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["class PositionalEncoding(T.nn.Module):\n","  # assumes inpt is [seq, bat, emb] - which word, \n","  # which sentence, which embed_val -- unintuitive\n","\n","  def __init__(self, d_model, drop_p=0.1, max_len=5000):\n","    super(PositionalEncoding, self).__init__()\n","    self.dropout = T.nn.Dropout(p=drop_p)\n","\n","    pe = T.zeros(max_len, d_model).to(device)\n","    position = T.arange(0, max_len, dtype=T.float).\\\n","      unsqueeze(1).to(device)\n","    div_term = T.exp(T.arange(0, d_model, 2).float() * \\\n","      (-math.log(10000.0) / d_model))\n","    pe[:, 0::2] = T.sin(position * div_term)\n","    pe[:, 1::2] = T.cos(position * div_term)\n","    self.pe = pe.unsqueeze(0).transpose(0, 1)\n","\n","  def forward(self, x):\n","    x = x + self.pe[:x.size(0), :]\n","    return self.dropout(x)"],"metadata":{"id":"8OkdAskMzudr","executionInfo":{"status":"ok","timestamp":1649767551090,"user_tz":-540,"elapsed":298,"user":{"displayName":"harasho amplil","userId":"06889073562372060382"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["print(\"\\nBegin \")\n","\n","# 0. get ready\n","T.manual_seed(1)\n","np.random.seed(1)\n","\n","# 2. create IMDB data iterators\n","print(\"\\nGetting (pruned) IMDB data and building vocabulary \")\n","# setting batch_first=False . . sets [seq,bat] required shape\n","\n","max_vocab_tokens = 500\n","bat_size = 3\n","\n","TEXT = tt.legacy.data.Field(lower=True, include_lengths=True,\n","  batch_first=False, tokenize=\"basic_english\")  # note!\n","LABEL = tt.legacy.data.Field(sequential=False)\n","\n","train_ds, test_ds = tt.legacy.datasets.IMDB.splits(TEXT, LABEL)\n","TEXT.build_vocab(train_ds, max_size=max_vocab_tokens-2) \n","LABEL.build_vocab(train_ds)  # unk, neg, pos\n","\n","train_itr, test_itr = \\\n","  tt.legacy.data.BucketIterator.splits((train_ds, test_ds), \\\n","  shuffle=True, batch_size=bat_size, device=device)\n","print(\"Data successfully fetched\")\n","\n","# 3. ceate model\n","print(\"\\nCreating TransformerModel object \")\n","\n","n_tokens = len(TEXT.vocab.stoi) \n","embed_dim = 4                \n","n_heads = 2     \n","n_hid = 10    \n","n_eclayers = 2  \n","drop_p = 0.2   \n","\n","model = TransformerModel(n_tokens, embed_dim, n_heads, \\\n","  n_hid, n_eclayers, drop_p).to(device)\n","print(\"Model successfully created \")\n","\n","# 4. feed a batch to model, display shapes as processing\n","\n","lr = 0.0\n","# lr_warmup = 100 \n","# gradient_clip = 1.0 \n","max_seq_len = 5\n","max_epochs = 3\n","\n","# loss_func = T.nn.CrossEntropyLoss()\n","# loss_func = T.nn.functional.nll_loss\n","# opt = T.optim.Adam(lr=arg_lr, params=model.parameters())\n","# sch = T.optim.lr_scheduler.LambdaLR(opt, lambda i: \\\n","#   min(i / (arg_lr_warmup / arg_batch_size), 1.0))\n","# opt = T.optim.SGD(model.parameters(), lr=lr)\n","\n","model.train()  # set mode\n","print(\"\\nStarting training (not really)\")\n","for epoch in range(max_epochs):\n","  print(\"\\nEpoch \" + str(epoch))\n","\n","  for bat_idx, batch in enumerate(train_itr):\n","    print(\" batch: \" + str(bat_idx))\n","    # optimizer.zero_grad()\n","\n","    inpt = batch.text[0]            # [seq, bat] required\n","    if len(inpt) != max_seq_len:  # trim inpt to 5 tokens\n","      inpt = inpt[0:max_seq_len, :]\n","    lbl = (batch.label - 1)         # 0, 1\n","    print(\"  targets: \")\n","    print(\"  \", end=\"\"); print(lbl); input()\n","\n","    # call forward(), display intermediate values and shapes\n","    oupt = model(inpt, None)        # no mask to forward()\n","    \n","    # training:\n","    # compute loss\n","    # call backward() to compute gradients\n","    # clip_grad_norm()\n","    # call step() to update wts and biases\n","    # loss = loss_func(output.view(-1, ntokens), targets)\n","    # loss.backward()\n","    # T.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n","    # optimizer.step()\n","\n","print(\"\\nEnd demo \")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":697},"id":"VTUNBqjfzwyq","executionInfo":{"status":"error","timestamp":1649769451264,"user_tz":-540,"elapsed":26739,"user":{"displayName":"harasho amplil","userId":"06889073562372060382"}},"outputId":"98f70eb4-5e6b-44c7-febe-d404f20bdca5"},"execution_count":9,"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Begin \n","\n","Getting (pruned) IMDB data and building vocabulary \n","Data successfully fetched\n","\n","Creating TransformerModel object \n","Model successfully created \n","\n","Starting training (not really)\n","\n","Epoch 0\n"," batch: 0\n","  targets: \n","  tensor([1, 0, 0])\n","abc\n"]},{"output_type":"error","ename":"NotImplementedError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-2d38a22d983a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# call forward(), display intermediate values and shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0moupt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# no mask to forward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;31m# training:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_forward_unimplemented\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mregistered\u001b[0m \u001b[0mhooks\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatter\u001b[0m \u001b[0msilently\u001b[0m \u001b[0mignores\u001b[0m \u001b[0mthem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \"\"\"\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: "]}]},{"cell_type":"code","source":[""],"metadata":{"id":"kH-8tV7u66Q-"},"execution_count":null,"outputs":[]}]}