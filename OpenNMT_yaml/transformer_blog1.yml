# 「Transformerを用いたニューラル機械翻訳をやってみよう」を参考にしたyaml

# preprocess.py
train_src: train.en.atok 
train_tgt: train.ja.atok 
valid_src: dev.en.atok 
valid_tgt: dev.ja.atok 
save_data: preprocessed_dataset

# train.py
data: preprocessed_dataset 
save_model: save_model_name
layers: 6
rnn_size: 512
word_vec_size: 512
transformer_ff: 2048
heads: 8
encoder_type: transformer
decoder_type: transformer
position_encoding: 'true'
train_steps: 200000
max_generator_batches: 2
dropout: 0.1
batch_size: 4096
batch_type: tokens
normalization: tokens
accum_count: 2 
optim: adam 
adam_beta2: 0.998 
decay_method: noam
warmup_steps: 8000 
learning_rate: 2 
max_grad_norm: 0 
param_init: 0 
param_init_glorot: 'true'
label_smoothing: 0.1 
valid_steps: 10000
save_checkpoint_steps: 40000 
world_size: 1 
gpu_ranks: 0 

# translateはそのままコマンド
