{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "opmnt_library_test.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1d0oiGBJlZNCPE3pBRJc2M0VdTAsvmJnn",
      "authorship_tag": "ABX9TyNT8p5oTMdrB+mgUeOetTTv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amplil/nlp-test/blob/master/opmnt_library_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVB_lU8MLAov"
      },
      "source": [
        "# 初期化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1DS1aRKLoEE"
      },
      "source": [
        "## インストール"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClLRYsiVAvmO"
      },
      "source": [
        "cd \"/content/drive/MyDrive/Colab Notebooks\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djsObUCTBuU5"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxJOKCR4BBvK"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/Colab Notebooks/packages\")\n",
        "import slacknotice # オリジナルモジュール"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09dBHBy5BCMh"
      },
      "source": [
        "pip install OpenNMT-py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysF-LuLqCvGj"
      },
      "source": [
        "!pip install PyYaml==5.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4JYVJIeLIjV"
      },
      "source": [
        "## インポート"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm-x7Md0BNZi"
      },
      "source": [
        "import yaml\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from argparse import Namespace\n",
        "from collections import defaultdict, Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwrLKwamBOAh"
      },
      "source": [
        "import onmt\n",
        "from onmt.inputters.inputter import _load_vocab, _build_fields_vocab, get_fields, IterOnDevice\n",
        "from onmt.inputters.corpus import ParallelCorpus\n",
        "from onmt.inputters.dynamic_iterator import DynamicDatasetIter\n",
        "from onmt.translate import GNMTGlobalScorer, Translator, TranslationBuilder\n",
        "from onmt.utils.misc import set_random_seed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l41tC3UGMs9a"
      },
      "source": [
        "# Enable logging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWCCFRMcBUgM"
      },
      "source": [
        "# enable logging\n",
        "from onmt.utils.logging import init_logger, logger\n",
        "init_logger()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4anv-bLMs9d"
      },
      "source": [
        "# Set random seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j428EmWOFoJQ"
      },
      "source": [
        "is_cuda = torch.cuda.is_available()\n",
        "set_random_seed(1111, is_cuda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cl_6LcKhGj6R"
      },
      "source": [
        "!ls toy-ende"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oqizNLVL8T2"
      },
      "source": [
        "# Prepare data and vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAClOkErHnvA"
      },
      "source": [
        "yaml_config = \"\"\"\n",
        "## Where the samples will be written\n",
        "save_data: toy-ende/run/example\n",
        "## Where the vocab(s) will be written\n",
        "src_vocab: toy-ende/run/example.vocab.src\n",
        "tgt_vocab: toy-ende/run/example.vocab.tgt\n",
        "# Corpus opts:\n",
        "data:\n",
        "    corpus:\n",
        "        path_src: toy-ende/src-train.txt\n",
        "        path_tgt: toy-ende/tgt-train.txt\n",
        "        transforms: []\n",
        "        weight: 1\n",
        "    valid:\n",
        "        path_src: toy-ende/src-val.txt\n",
        "        path_tgt: toy-ende/tgt-val.txt\n",
        "        transforms: []\n",
        "\"\"\"\n",
        "config = yaml.safe_load(yaml_config)\n",
        "with open(\"toy-ende/config.yaml\", \"w\") as f:\n",
        "    f.write(yaml_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGVaaUACIMze"
      },
      "source": [
        "\"\"\"\n",
        "from onmt.utils.parse import ArgumentParser\n",
        "parser = DynamicArgumentParser(description='build_vocab.py')\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yhu0TwfTJ6SG"
      },
      "source": [
        "from onmt.utils.parse import ArgumentParser\n",
        "parser = ArgumentParser(description='build_vocab.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FlbWDAcIa9d"
      },
      "source": [
        "from onmt.opts import dynamic_prepare_opts\n",
        "dynamic_prepare_opts(parser, build_vocab_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An_UeSFpKOkt"
      },
      "source": [
        "base_args = ([\"-config\", \"toy-ende/config.yaml\", \"-n_sample\", \"10000\"])\n",
        "opts, unknown = parser.parse_known_args(base_args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw9VLtnQKUIm"
      },
      "source": [
        "opts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyKTApt3KVg-"
      },
      "source": [
        "from onmt.bin.build_vocab import build_vocab_main\n",
        "build_vocab_main(opts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OTbR5RnMs9n"
      },
      "source": [
        "# Build fields"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6cnWxFMMs9n"
      },
      "source": [
        "We can build the fields from the text files that were just created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba1WJm6bMs9o"
      },
      "source": [
        "src_vocab_path = \"toy-ende/run/example.vocab.src\"\n",
        "tgt_vocab_path = \"toy-ende/run/example.vocab.tgt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOOR5p1AMs9o"
      },
      "source": [
        "# initialize the frequency counter\n",
        "counters = defaultdict(Counter)\n",
        "# load source vocab\n",
        "_src_vocab, _src_vocab_size = _load_vocab(\n",
        "    src_vocab_path,\n",
        "    'src',\n",
        "    counters)\n",
        "# load target vocab\n",
        "_tgt_vocab, _tgt_vocab_size = _load_vocab(\n",
        "    tgt_vocab_path,\n",
        "    'tgt',\n",
        "    counters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hedm3126Ms9o"
      },
      "source": [
        "# initialize fields\n",
        "src_nfeats, tgt_nfeats = 0, 0 # do not support word features for now\n",
        "fields = get_fields(\n",
        "    'text', src_nfeats, tgt_nfeats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1k62mZEMs9p"
      },
      "source": [
        "fields"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ8eTfZWMs9p"
      },
      "source": [
        "# build fields vocab\n",
        "share_vocab = False\n",
        "vocab_size_multiple = 1\n",
        "src_vocab_size = 30000\n",
        "tgt_vocab_size = 30000\n",
        "src_words_min_frequency = 1\n",
        "tgt_words_min_frequency = 1\n",
        "vocab_fields = _build_fields_vocab(\n",
        "    fields, counters, 'text', share_vocab,\n",
        "    vocab_size_multiple,\n",
        "    src_vocab_size, src_words_min_frequency,\n",
        "    tgt_vocab_size, tgt_words_min_frequency)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xj7KJdxtMs9p"
      },
      "source": [
        "An alternative way of creating these fields is to run `onmt_train` without actually training, to just output the necessary files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYmcLhgIMs9q"
      },
      "source": [
        "# Prepare for training: model and optimizer creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXhQm3GaMs9q"
      },
      "source": [
        "Let's get a few fields/vocab related variables to simplify the model creation a bit:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OachNuJ4Ms9q"
      },
      "source": [
        "src_text_field = vocab_fields[\"src\"].base_field\n",
        "src_vocab = src_text_field.vocab\n",
        "src_padding = src_vocab.stoi[src_text_field.pad_token]\n",
        "\n",
        "tgt_text_field = vocab_fields['tgt'].base_field\n",
        "tgt_vocab = tgt_text_field.vocab\n",
        "tgt_padding = tgt_vocab.stoi[tgt_text_field.pad_token]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEZa8AI_Ms9r"
      },
      "source": [
        "Next we specify the core model itself. Here we will build a small model with an encoder and an attention based input feeding decoder. Both models will be RNNs and the encoder will be bidirectional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAt6MQa6Ms9r"
      },
      "source": [
        "emb_size = 100\n",
        "rnn_size = 500\n",
        "# Specify the core model.\n",
        "\n",
        "encoder_embeddings = onmt.modules.Embeddings(emb_size, len(src_vocab),\n",
        "                                             word_padding_idx=src_padding)\n",
        "\n",
        "encoder = onmt.encoders.RNNEncoder(hidden_size=rnn_size, num_layers=1,\n",
        "                                   rnn_type=\"LSTM\", bidirectional=True,\n",
        "                                   embeddings=encoder_embeddings)\n",
        "\n",
        "decoder_embeddings = onmt.modules.Embeddings(emb_size, len(tgt_vocab),\n",
        "                                             word_padding_idx=tgt_padding)\n",
        "decoder = onmt.decoders.decoder.InputFeedRNNDecoder(\n",
        "    hidden_size=rnn_size, num_layers=1, bidirectional_encoder=True, \n",
        "    rnn_type=\"LSTM\", embeddings=decoder_embeddings)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = onmt.models.model.NMTModel(encoder, decoder)\n",
        "model.to(device)\n",
        "\n",
        "# Specify the tgt word generator and loss computation module\n",
        "model.generator = nn.Sequential(\n",
        "    nn.Linear(rnn_size, len(tgt_vocab)),\n",
        "    nn.LogSoftmax(dim=-1)).to(device)\n",
        "\n",
        "loss = onmt.utils.loss.NMTLossCompute(\n",
        "    criterion=nn.NLLLoss(ignore_index=tgt_padding, reduction=\"sum\"),\n",
        "    generator=model.generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbSN0_woMs9s"
      },
      "source": [
        "Now we set up the optimizer. This could be a core torch optim class, or our wrapper which handles learning rate updates and gradient normalization automatically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3mxhNIqMs9s"
      },
      "source": [
        "lr = 1\n",
        "torch_optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "optim = onmt.utils.optimizers.Optimizer(\n",
        "    torch_optimizer, learning_rate=lr, max_grad_norm=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOCxshmVMs9s"
      },
      "source": [
        "# Create the training and validation data iterators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdwJZ4MIMs9t"
      },
      "source": [
        "Now we need to create the dynamic dataset iterator.\n",
        "\n",
        "This is not very 'library-friendly' for now because of the way the `DynamicDatasetIter` constructor is defined. It may evolve in the future."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W9OhOGtMs9t"
      },
      "source": [
        "src_train = \"toy-ende/src-train.txt\"\n",
        "tgt_train = \"toy-ende/tgt-train.txt\"\n",
        "src_val = \"toy-ende/src-val.txt\"\n",
        "tgt_val = \"toy-ende/tgt-val.txt\"\n",
        "\n",
        "# build the ParallelCorpus\n",
        "corpus = ParallelCorpus(\"corpus\", src_train, tgt_train)\n",
        "valid = ParallelCorpus(\"valid\", src_val, tgt_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhYT9QDTMs9t"
      },
      "source": [
        "# build the training iterator\n",
        "train_iter = DynamicDatasetIter(\n",
        "    corpora={\"corpus\": corpus},\n",
        "    corpora_info={\"corpus\": {\"weight\": 1}},\n",
        "    transforms={},\n",
        "    fields=vocab_fields,\n",
        "    is_train=True,\n",
        "    batch_type=\"tokens\",\n",
        "    batch_size=4096,\n",
        "    batch_size_multiple=1,\n",
        "    data_type=\"text\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9NwAFn5Ms9t"
      },
      "source": [
        "# make sure the iteration happens on GPU 0 (-1 for CPU, N for GPU N)\n",
        "train_iter = iter(IterOnDevice(train_iter, 0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Axo3OYIXMs9u"
      },
      "source": [
        "# build the validation iterator\n",
        "valid_iter = DynamicDatasetIter(\n",
        "    corpora={\"valid\": valid},\n",
        "    corpora_info={\"valid\": {\"weight\": 1}},\n",
        "    transforms={},\n",
        "    fields=vocab_fields,\n",
        "    is_train=False,\n",
        "    batch_type=\"sents\",\n",
        "    batch_size=8,\n",
        "    batch_size_multiple=1,\n",
        "    data_type=\"text\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7abV3Rz9Ms9u"
      },
      "source": [
        "valid_iter = IterOnDevice(valid_iter, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHw4VLQdMs9u"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bTvt94KMs9u"
      },
      "source": [
        "Finally we train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sklpMVNHMs9u"
      },
      "source": [
        "report_manager = onmt.utils.ReportMgr(\n",
        "    report_every=50, start_time=None, tensorboard_writer=None)\n",
        "\n",
        "trainer = onmt.Trainer(model=model,\n",
        "                       train_loss=loss,\n",
        "                       valid_loss=loss,\n",
        "                       optim=optim,\n",
        "                       report_manager=report_manager,\n",
        "                       dropout=[0.1])\n",
        "\n",
        "trainer.train(train_iter=train_iter,\n",
        "              train_steps=1000,\n",
        "              valid_iter=valid_iter,\n",
        "              valid_steps=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkg_p_naMs9v"
      },
      "source": [
        "# Translate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_UcBBKSMs9v"
      },
      "source": [
        "For translation, we can build a \"traditional\" (as opposed to dynamic) dataset for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8HtVdUTcn9F"
      },
      "source": [
        "src_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upKnV0hKdykr"
      },
      "source": [
        "src_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwmZS7-_Ms9v"
      },
      "source": [
        "src_data = {\"reader\": onmt.inputters.str2reader[\"text\"](), \"data\": src_val}\n",
        "tgt_data = {\"reader\": onmt.inputters.str2reader[\"text\"](), \"data\": tgt_val}\n",
        "_readers, _data = onmt.inputters.Dataset.config(\n",
        "    [('src', src_data), ('tgt', tgt_data)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-ENWBk-Ms9v"
      },
      "source": [
        "dataset = onmt.inputters.Dataset(\n",
        "    vocab_fields, readers=_readers, data=_data,\n",
        "    sort_key=onmt.inputters.str2sortkey[\"text\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsFTYhaMMs9w"
      },
      "source": [
        "data_iter = onmt.inputters.OrderedIterator(\n",
        "            dataset=dataset,\n",
        "            device=\"cuda\",\n",
        "            batch_size=10,\n",
        "            train=False,\n",
        "            sort=False,\n",
        "            sort_within_batch=True,\n",
        "            shuffle=False\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZals7N3Ms9w"
      },
      "source": [
        "src_reader = onmt.inputters.str2reader[\"text\"]\n",
        "tgt_reader = onmt.inputters.str2reader[\"text\"]\n",
        "scorer = GNMTGlobalScorer(alpha=0.7, \n",
        "                          beta=0., \n",
        "                          length_penalty=\"avg\", \n",
        "                          coverage_penalty=\"none\")\n",
        "gpu = 0 if torch.cuda.is_available() else -1\n",
        "translator = Translator(model=model, \n",
        "                        fields=vocab_fields, \n",
        "                        src_reader=src_reader, \n",
        "                        tgt_reader=tgt_reader, \n",
        "                        global_scorer=scorer,\n",
        "                        gpu=gpu)\n",
        "builder = onmt.translate.TranslationBuilder(data=dataset, \n",
        "                                            fields=vocab_fields)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TXttZeLMs9w"
      },
      "source": [
        "**Note**: translations will be very poor, because of the very low quantity of data, the absence of proper tokenization, and the brevity of the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYhzEq8TMs9w"
      },
      "source": [
        "for batch in data_iter:\n",
        "    trans_batch = translator.translate_batch(\n",
        "        batch=batch, src_vocabs=[src_vocab],\n",
        "        attn_debug=False)\n",
        "    translations = builder.from_batch(trans_batch)\n",
        "    for trans in translations:\n",
        "        print(trans.log(0))\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}