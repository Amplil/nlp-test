{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "opmnt_library_transformer_translate.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1vzqvDktpgYnnGsvn6WAWsTgOP2y1dD1z",
      "authorship_tag": "ABX9TyO68oD+hmvhCTJ3SH8UlfV5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amplil/nlp-test/blob/master/opmnt_library_transformer_translate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVB_lU8MLAov"
      },
      "source": [
        "# 初期化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClLRYsiVAvmO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14546166-54c4-455e-83e4-30403f4f379d"
      },
      "source": [
        "cd \"/content/drive/MyDrive/Colab Notebooks\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1liJWbv2EhngAj7pixTjzRpv9B-Bz9zCt/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djsObUCTBuU5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a47a7049-da5b-4342-b573-63b5e7d1ffe2"
      },
      "source": [
        "ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;36mdata\u001b[0m@                                      pred.txt\n",
            "new_colab2.ipynb                           \u001b[01;36mtoy-ende\u001b[0m@\n",
            "opmnt_library_test.ipynb                   \u001b[01;36mTransformer\u001b[0m@\n",
            "opmnt_library_transformer_translate.ipynb  transformer_kyoto_ja_en.ipynb\n",
            "\u001b[01;36mpackages\u001b[0m@                                  transformer_test.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxJOKCR4BBvK"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/Colab Notebooks/packages\")\n",
        "import slacknotice # オリジナルモジュール"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09dBHBy5BCMh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3652933-f07a-4ce5-d00e-8364356143ec"
      },
      "source": [
        "pip install OpenNMT-py"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting OpenNMT-py\n",
            "  Downloading OpenNMT_py-2.2.0-py3-none-any.whl (216 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▌                              | 10 kB 18.8 MB/s eta 0:00:01\r\u001b[K     |███                             | 20 kB 23.7 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 30 kB 20.0 MB/s eta 0:00:01\r\u001b[K     |██████                          | 40 kB 16.2 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 51 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 61 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 81 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 92 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 102 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 112 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 122 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 133 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 143 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 153 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 163 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 174 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 184 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 194 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 204 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 215 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 216 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py) (1.1.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py) (3.13)\n",
            "Collecting pyonmttok<2,>=1.23\n",
            "  Downloading pyonmttok-1.29.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (16.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.0 MB 117 kB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py) (1.9.0+cu111)\n",
            "Collecting torchtext==0.5.0\n",
            "  Downloading torchtext-0.5.0-py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.3 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py) (2.6.0)\n",
            "Collecting waitress\n",
            "  Downloading waitress-2.0.0-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting configargparse\n",
            "  Downloading ConfigArgParse-1.5.3-py3-none-any.whl (20 kB)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 44.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py) (4.62.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.4.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.37.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.12.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.6.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.41.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.35.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3->OpenNMT-py) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.3->OpenNMT-py) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py) (2021.5.30)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3->OpenNMT-py) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->OpenNMT-py) (3.7.4.3)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py) (1.1.0)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py) (7.1.2)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->OpenNMT-py) (2.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=2.3->OpenNMT-py) (3.6.0)\n",
            "Installing collected packages: sentencepiece, waitress, torchtext, pyonmttok, configargparse, OpenNMT-py\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.10.0\n",
            "    Uninstalling torchtext-0.10.0:\n",
            "      Successfully uninstalled torchtext-0.10.0\n",
            "Successfully installed OpenNMT-py-2.2.0 configargparse-1.5.3 pyonmttok-1.29.0 sentencepiece-0.1.96 torchtext-0.5.0 waitress-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm-x7Md0BNZi"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from argparse import Namespace\n",
        "from collections import defaultdict, Counter"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwrLKwamBOAh"
      },
      "source": [
        "import onmt\n",
        "from onmt.inputters.inputter import _load_vocab, _build_fields_vocab, get_fields, IterOnDevice\n",
        "from onmt.inputters.corpus import ParallelCorpus\n",
        "from onmt.inputters.dynamic_iterator import DynamicDatasetIter\n",
        "from onmt.translate import GNMTGlobalScorer, Translator, TranslationBuilder\n",
        "from onmt.utils.misc import set_random_seed"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWCCFRMcBUgM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e313e5cf-b38b-4cfe-adfa-5ad13a857965"
      },
      "source": [
        "# enable logging\n",
        "from onmt.utils.logging import init_logger, logger\n",
        "init_logger()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<RootLogger root (INFO)>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtInB14qBNkU"
      },
      "source": [
        "# モデルの読み込み"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba1WJm6bMs9o"
      },
      "source": [
        "src_vocab_path = \"data/kftt-data-1.0/run/example.vocab.src\"\n",
        "tgt_vocab_path = \"data/kftt-data-1.0/run/example.vocab.tgt\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOOR5p1AMs9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f0dd6b2-a87f-4b45-b5b3-070383ffc837"
      },
      "source": [
        "# initialize the frequency counter\n",
        "counters = defaultdict(Counter)\n",
        "# load source vocab\n",
        "_src_vocab, _src_vocab_size = _load_vocab(\n",
        "    src_vocab_path,\n",
        "    'src',\n",
        "    counters)\n",
        "# load target vocab\n",
        "_tgt_vocab, _tgt_vocab_size = _load_vocab(\n",
        "    tgt_vocab_path,\n",
        "    'tgt',\n",
        "    counters)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2021-11-07 06:03:27,318 INFO] Loading src vocabulary from data/kftt-data-1.0/run/example.vocab.src\n",
            "[2021-11-07 06:03:28,621 INFO] Loaded src vocab has 13637 tokens.\n",
            "[2021-11-07 06:03:28,635 INFO] Loading tgt vocabulary from data/kftt-data-1.0/run/example.vocab.tgt\n",
            "[2021-11-07 06:03:28,900 INFO] Loaded tgt vocab has 12199 tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hedm3126Ms9o"
      },
      "source": [
        "# initialize fields\n",
        "src_nfeats, tgt_nfeats = 0, 0 # do not support word features for now\n",
        "fields = get_fields(\n",
        "    'text', src_nfeats, tgt_nfeats)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ8eTfZWMs9p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5de903c4-05ba-4508-f323-acb5e9ee9c93"
      },
      "source": [
        "# build fields vocab\n",
        "share_vocab = False\n",
        "vocab_size_multiple = 1\n",
        "src_vocab_size = 30000\n",
        "tgt_vocab_size = 30000\n",
        "src_words_min_frequency = 1\n",
        "tgt_words_min_frequency = 1\n",
        "vocab_fields = _build_fields_vocab(\n",
        "    fields, counters, 'text', share_vocab,\n",
        "    vocab_size_multiple,\n",
        "    src_vocab_size, src_words_min_frequency,\n",
        "    tgt_vocab_size, tgt_words_min_frequency)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2021-11-07 06:03:28,971 INFO]  * tgt vocab size: 12203.\n",
            "[2021-11-07 06:03:28,993 INFO]  * src vocab size: 13639.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5h-H_GscMCW"
      },
      "source": [
        "src_val = \"data/kftt-data-1.0/data/tok/kyoto-test.en\"\n",
        "tgt_val = \"data/kftt-data-1.0/data/tok/kyoto-test.ja\""
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VwMTv86bbaC"
      },
      "source": [
        "\"\"\"\n",
        "src_data = {\"reader\": onmt.inputters.str2reader[\"text\"](), \"data\": src_val}\n",
        "tgt_data = {\"reader\": onmt.inputters.str2reader[\"text\"](), \"data\": tgt_val}\n",
        "_readers, _data = onmt.inputters.Dataset.config(\n",
        "    [('src', src_data), ('tgt', tgt_data)])\n",
        "\"\"\"\n",
        "src_data = {\n",
        "    \"reader\": onmt.inputters.str2reader[\"text\"](),\n",
        "    \"data\": src_val,\n",
        "    \"features\": {}\n",
        "}\n",
        "tgt_data = {\n",
        "    \"reader\": onmt.inputters.str2reader[\"text\"](),\n",
        "    \"data\": tgt_val,\n",
        "    \"features\": {}\n",
        "}\n",
        "_readers, _data = onmt.inputters.Dataset.config(\n",
        "    [(\"src\", src_data), (\"tgt\", tgt_data)]\n",
        ")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "hmkYmSnoD_W_",
        "outputId": "670a55f1-5382-46e6-ba9e-c87c73452c7f"
      },
      "source": [
        "opt.vocab_fields"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-a9bafc4bae18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_fields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'Namespace' object has no attribute 'vocab_fields'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ftj-avJTFsPC",
        "outputId": "9c619f52-f042-4898-d1af-88ea8858fb86"
      },
      "source": [
        "opt.src"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'data/kftt-data-1.0/data/tok/kyoto-test.en'"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v1fFmqkFy1U",
        "outputId": "6641c0fd-4ce1-4bc7-e2af-a39d894b6c2f"
      },
      "source": [
        "opt.shard_size"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llduwf7iGF0S",
        "outputId": "d57d15e3-4bab-4c93-88f9-0f74fe7656d8"
      },
      "source": [
        "opt.attn_debug"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jbYJTuEOQIM"
      },
      "source": [
        "op"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "Scgh_pf7GfST",
        "outputId": "4e271341-8429-4039-e7c3-450431def8dd"
      },
      "source": [
        "load_test_model(opt)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-68d609ba9e9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mload_test_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'load_test_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgiK-yvrbk_k"
      },
      "source": [
        "dataset = onmt.inputters.Dataset(\n",
        "    vocab_fields, readers=_readers, data=_data,\n",
        "    sort_key=onmt.inputters.str2sortkey[\"text\"])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0RMWuGdbnNG"
      },
      "source": [
        "data_iter = onmt.inputters.OrderedIterator(\n",
        "            dataset=dataset,\n",
        "            device=\"cuda\",\n",
        "            batch_size=10,\n",
        "            train=False,\n",
        "            sort=False,\n",
        "            sort_within_batch=True,\n",
        "            shuffle=False\n",
        "        )"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLrOgHB8OsTm"
      },
      "source": [
        "src_text_field = vocab_fields[\"src\"].base_field\n",
        "src_vocab = src_text_field.vocab\n",
        "src_padding = src_vocab.stoi[src_text_field.pad_token]\n",
        "\n",
        "tgt_text_field = vocab_fields['tgt'].base_field\n",
        "tgt_vocab = tgt_text_field.vocab\n",
        "tgt_padding = tgt_vocab.stoi[tgt_text_field.pad_token]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7m2TQ0MOXuX"
      },
      "source": [
        "# LSTMでの翻訳\n",
        "\"\"\"\n",
        "emb_size = 100\n",
        "rnn_size = 500\n",
        "# Specify the core model.\n",
        "\n",
        "encoder_embeddings = onmt.modules.Embeddings(emb_size, len(src_vocab),\n",
        "                                             word_padding_idx=src_padding)\n",
        "\n",
        "encoder = onmt.encoders.RNNEncoder(hidden_size=rnn_size, num_layers=1,\n",
        "                                   rnn_type=\"LSTM\", bidirectional=True,\n",
        "                                   embeddings=encoder_embeddings)\n",
        "\n",
        "decoder_embeddings = onmt.modules.Embeddings(emb_size, len(tgt_vocab),\n",
        "                                             word_padding_idx=tgt_padding)\n",
        "decoder = onmt.decoders.decoder.InputFeedRNNDecoder(\n",
        "    hidden_size=rnn_size, num_layers=1, bidirectional_encoder=True, \n",
        "    rnn_type=\"LSTM\", embeddings=decoder_embeddings)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = onmt.models.model.NMTModel(encoder, decoder)\n",
        "model.to(device)\n",
        "\n",
        "# Specify the tgt word generator and loss computation module\n",
        "model.generator = nn.Sequential(\n",
        "    nn.Linear(rnn_size, len(tgt_vocab)),\n",
        "    nn.LogSoftmax(dim=-1)).to(device)\n",
        "\n",
        "loss = onmt.utils.loss.NMTLossCompute(\n",
        "    criterion=nn.NLLLoss(ignore_index=tgt_padding, reduction=\"sum\"),\n",
        "    generator=model.generator)\n",
        "\"\"\""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_xa0DA0bpIU"
      },
      "source": [
        "src_reader = onmt.inputters.str2reader[\"text\"]\n",
        "tgt_reader = onmt.inputters.str2reader[\"text\"]\n",
        "scorer = GNMTGlobalScorer(alpha=0.7, \n",
        "                          beta=0., \n",
        "                          length_penalty=\"avg\", \n",
        "                          coverage_penalty=\"none\")\n",
        "gpu = 0 if torch.cuda.is_available() else -1\n",
        "translator = Translator(model=model, \n",
        "                        fields=vocab_fields, \n",
        "                        src_reader=src_reader, \n",
        "                        tgt_reader=tgt_reader, \n",
        "                        global_scorer=scorer,\n",
        "                        gpu=gpu)\n",
        "builder = onmt.translate.TranslationBuilder(data=dataset, \n",
        "                                            fields=vocab_fields)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umQJYyFWsvAy",
        "outputId": "791bb593-3090-4639-d7da-06fd10387b48"
      },
      "source": [
        "import onmt.opts as opts\n",
        "from onmt.utils.parse import ArgumentParser\n",
        "#from onmt.opts import dynamic_prepare_opts\n",
        "\n",
        "parser = ArgumentParser(description='translate.py')\n",
        "#dynamic_prepare_opts(parser, build_vocab_only=True)\n",
        "opts.translate_opts(parser)\n",
        "\n",
        "base_args = ([\"-model\", \"data/kftt-data-1.0/run/model_step_15000.pt\", \"-src\", \"data/kftt-data-1.0/data/tok/kyoto-test.en\", \"-output\", \"data/kftt-data-1.0/out.txt\", \"-gpu\", \"0\", \"-verbose\"])\n",
        "opt, unknown = parser.parse_known_args(base_args)\n",
        "opt"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Namespace(align_debug=False, alpha=0.0, attn_debug=False, avg_raw_probs=False, ban_unk_token=False, batch_size=30, batch_type='sents', beam_size=5, beta=-0.0, block_ngram_repeat=0, coverage_penalty='none', data_type='text', dump_beam='', fp32=False, gpu=0, ignore_when_blocking=[], int8=False, length_penalty='none', log_file='', log_file_level='0', max_length=100, max_sent_length=None, min_length=0, models=['data/kftt-data-1.0/run/model_step_15000.pt'], n_best=1, output='data/kftt-data-1.0/out.txt', phrase_table='', random_sampling_temp=1.0, random_sampling_topk=0, random_sampling_topp=0.0, ratio=-0.0, replace_unk=False, report_align=False, report_time=False, seed=-1, shard_size=10000, src='data/kftt-data-1.0/data/tok/kyoto-test.en', src_feats=None, stepwise_penalty=False, tgt=None, tgt_prefix=False, verbose=True)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHcg9gkUNs7V"
      },
      "source": [
        "from onmt.translate.translator import build_translator\n",
        "\n",
        "ArgumentParser.validate_translate_opts(opt)\n",
        "logger = init_logger(opt.log_file)\n",
        "\n",
        "translator = build_translator(opt, logger=logger, report_score=True)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1njxC7MJ8u_"
      },
      "source": [
        "# out_fileの作成\n",
        "\"\"\"\n",
        "if out_file is None:\n",
        "    out_file = codecs.open(opt.output, \"w+\", \"utf-8\")\n",
        "\"\"\"\n",
        "load_test_model = (\n",
        "    onmt.decoders.ensemble.load_test_model\n",
        "    if len(opt.models) > 1\n",
        "    else onmt.model_builder.load_test_model\n",
        ")\n",
        "vocab_fields, model, model_opt = load_test_model(opt) # fields->vocab_fields\n",
        "\n",
        "scorer = onmt.translate.GNMTGlobalScorer.from_opt(opt)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9WIABBdhY0K",
        "outputId": "cec010ea-93bb-47b7-a352-45b3c3bc3fda"
      },
      "source": [
        "#def load_test_model(opt, model_path=None)\n",
        "\"\"\"\n",
        "from onmt.utils.parse import ArgumentParser\n",
        "import onmt.inputters as inputters\n",
        "from onmt.utils.misc import use_gpu\n",
        "\n",
        "model_path=\"data/kftt-data-1.0/run/model_step_15000.pt\"\n",
        "\n",
        "checkpoint = torch.load(model_path,map_location=lambda storage, loc: storage)\n",
        "\n",
        "model_opt = ArgumentParser.ckpt_model_opts(checkpoint['opt'])\n",
        "ArgumentParser.update_model_opts(model_opt)\n",
        "ArgumentParser.validate_model_opts(model_opt)\n",
        "fields = checkpoint['vocab']\n",
        "\n",
        "# Avoid functionality on inference\n",
        "model_opt.update_vocab = False\n",
        "\n",
        "model = onmt.model_builder.build_base_model(model_opt, fields, use_gpu(opt), checkpoint,\n",
        "                            opt.gpu)\n",
        "if opt.fp32:\n",
        "    model.float()\n",
        "elif opt.int8:\n",
        "    if opt.gpu >= 0:\n",
        "        raise ValueError(\n",
        "            \"Dynamic 8-bit quantization is not supported on GPU\")\n",
        "    torch.quantization.quantize_dynamic(model, inplace=True)\n",
        "model.eval()\n",
        "model.generator.eval()\n",
        "\"\"\""
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=512, out_features=12203, bias=True)\n",
              "  (1): Cast()\n",
              "  (2): LogSoftmax(dim=-1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGDQWf9HgzVn",
        "outputId": "a741fdf6-ba70-40b6-da6d-734edcd6c4bf"
      },
      "source": [
        "model"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NMTModel(\n",
              "  (encoder): TransformerEncoder(\n",
              "    (embeddings): Embeddings(\n",
              "      (make_embedding): Sequential(\n",
              "        (emb_luts): Elementwise(\n",
              "          (0): Embedding(13639, 512, padding_idx=1)\n",
              "        )\n",
              "        (pe): PositionalEncoding(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (transformer): ModuleList(\n",
              "      (0): TransformerEncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (softmax): Softmax(dim=-1)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): TransformerEncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (softmax): Softmax(dim=-1)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): TransformerEncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (softmax): Softmax(dim=-1)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (3): TransformerEncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (softmax): Softmax(dim=-1)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (4): TransformerEncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (softmax): Softmax(dim=-1)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (5): TransformerEncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (softmax): Softmax(dim=-1)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "  )\n",
              "  (decoder): TransformerDecoder(\n",
              "    (embeddings): Embeddings(\n",
              "      (make_embedding): Sequential(\n",
              "        (emb_luts): Elementwise(\n",
              "          (0): Embedding(12203, 512, padding_idx=1)\n",
              "        )\n",
              "        (pe): PositionalEncoding(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "    (transformer_layers): ModuleList(\n",
              "      (0): TransformerDecoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (softmax): Softmax(dim=-1)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "        (context_attn): MultiHeadedAttention(\n",
              "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (softmax): Softmax(dim=-1)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "      (1): TransformerDecoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (softmax): Softmax(dim=-1)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "        (context_attn): MultiHeadedAttention(\n",
              "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (softmax): Softmax(dim=-1)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "      (2): TransformerDecoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (softmax): Softmax(dim=-1)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "        (context_attn): MultiHeadedAttention(\n",
              "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (softmax): Softmax(dim=-1)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "      (3): TransformerDecoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (softmax): Softmax(dim=-1)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "        (context_attn): MultiHeadedAttention(\n",
              "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (softmax): Softmax(dim=-1)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "      (4): TransformerDecoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (softmax): Softmax(dim=-1)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "        (context_attn): MultiHeadedAttention(\n",
              "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (softmax): Softmax(dim=-1)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "      (5): TransformerDecoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (softmax): Softmax(dim=-1)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "        (context_attn): MultiHeadedAttention(\n",
              "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (softmax): Softmax(dim=-1)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (generator): Sequential(\n",
              "    (0): Linear(in_features=512, out_features=12203, bias=True)\n",
              "    (1): Cast()\n",
              "    (2): LogSoftmax(dim=-1)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r08MpuSibprl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "e7daec04-87de-4034-be2c-df4f06758723"
      },
      "source": [
        "from onmt.constants import DefaultTokens\n",
        "from onmt.utils.misc import tile, set_random_seed, report_matrix\n",
        "\n",
        "# アテンションデバッグをON\n",
        "attn_debug=True\n",
        "for batch in data_iter:\n",
        "    trans_batch = translator.translate_batch(\n",
        "        batch=batch, src_vocabs=[src_vocab],\n",
        "        attn_debug=attn_debug)\n",
        "    translations = builder.from_batch(trans_batch)\n",
        "    for trans in translations:\n",
        "        print(trans.log(0))\n",
        "        \n",
        "        if attn_debug:\n",
        "            preds = trans.pred_sents[0]\n",
        "            preds.append(DefaultTokens.EOS)\n",
        "            attns = trans.attns[0].tolist()\n",
        "            if translator.data_type == \"text\":\n",
        "                srcs = trans.src_raw\n",
        "            else:\n",
        "                srcs = [str(item) for item in range(len(attns[0]))]\n",
        "            output = report_matrix(srcs, preds, attns)\n",
        "            if translator.logger:\n",
        "                translator.logger.info(output)\n",
        "            else:\n",
        "                #os.write(1, output.encode(\"utf-8\"))\n",
        "                print(output)\n",
        "    break"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-369fda836c53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     trans_batch = translator.translate_batch(\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_vocabs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msrc_vocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         attn_debug=attn_debug)\n\u001b[1;32m     10\u001b[0m     \u001b[0mtranslations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'src_vocab' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYHAc4NtRF3b"
      },
      "source": [
        "builder = onmt.translate.TranslationBuilder(data=dataset, fields=vocab_fields)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lm-GL0PZO5Us",
        "outputId": "ccb588cd-5fad-4f68-a32e-3ae7350bebee"
      },
      "source": [
        "from onmt.constants import DefaultTokens\n",
        "from onmt.utils.misc import tile, set_random_seed, report_matrix\n",
        "\n",
        "# アテンションデバッグをON\n",
        "attn_debug=True\n",
        "for batch in data_iter:\n",
        "    batch_data = translator.translate_batch(\n",
        "        batch, dataset.src_vocabs, attn_debug\n",
        "    )\n",
        "    translations = builder.from_batch(batch_data)\n",
        "\n",
        "    for trans in translations:\n",
        "        print(trans.log(0))\n",
        "        \n",
        "        if attn_debug:\n",
        "            preds = trans.pred_sents[0]\n",
        "            preds.append(DefaultTokens.EOS)\n",
        "            attns = trans.attns[0].tolist()\n",
        "            if translator.data_type == \"text\":\n",
        "                srcs = trans.src_raw\n",
        "            else:\n",
        "                srcs = [str(item) for item in range(len(attns[0]))]\n",
        "            output = report_matrix(srcs, preds, attns)\n",
        "            if translator.logger:\n",
        "                translator.logger.info(output)\n",
        "            else:\n",
        "                #os.write(1, output.encode(\"utf-8\"))\n",
        "                print(output)\n",
        "    break"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2021-11-07 07:21:11,285 INFO]               Infobox    Buddhis \n",
            "        仏教 *0.5843355  0.4156645 \n",
            "        用語 *0.9486390  0.0513610 \n",
            "      </s> *0.9673884  0.0326116 \n",
            "\n",
            "[2021-11-07 07:21:11,287 INFO]                 Dogen        was          a        Zen       monk         in        the      early    Kamakur     period          . \n",
            "        道元  0.0093218  0.0463321  0.0500570  0.1755526  0.0118073  0.0055185  0.0002921  0.0005321  0.0067596  0.0001872 *0.6936397 \n",
            "         は  0.0019194  0.0288408  0.0649257  0.0809890  0.0503864  0.0177336  0.0022550  0.0003598  0.0263847  0.0007763 *0.7254291 \n",
            "        鎌倉  0.0076908  0.1154914  0.1769026  0.1894506  0.0145433  0.0271018  0.0070224  0.0105953  0.0028024  0.0022899 *0.4461096 \n",
            "        時代  0.0121528  0.0490180  0.0410148  0.1046753  0.0237850  0.0067809  0.0008257  0.0022857  0.0028120  0.0024362 *0.7542137 \n",
            "        初期  0.0100437  0.1463979  0.1431923  0.1536881  0.0408283  0.0317716  0.0045548  0.0061332  0.0052312  0.0059326 *0.4522263 \n",
            "         の  0.0051170  0.1284089  0.2717123  0.2695615  0.0358035  0.0099961  0.0015333  0.0010793  0.0036660  0.0007578 *0.2723642 \n",
            "        禅僧  0.0059661  0.0336128  0.0646940 *0.7257243  0.0324532  0.0031065  0.0002394  0.0005844  0.0051965  0.0001159  0.1283069 \n",
            "         。  0.0008101  0.1210781  0.1382017  0.0092422  0.0011096  0.0073052  0.0006318  0.0000659  0.0001133  0.0004280 *0.7210143 \n",
            "      </s>  0.0046223  0.0416591  0.0496248  0.1157430  0.0175067  0.0642352  0.0510230  0.0172721  0.0435150  0.0578044 *0.5369943 \n",
            "\n",
            "[2021-11-07 07:21:11,292 INFO]                   The    founder         of       Soto        Zen \n",
            "        曹洞  0.1406423  0.0088394  0.0614562  0.2235358 *0.5655263 \n",
            "         宗 *0.3879875  0.0365609  0.2023250  0.0171178  0.3560088 \n",
            "         の *0.6430637  0.0082695  0.0606325  0.0379367  0.2500976 \n",
            "        開祖  0.3550598  0.0286561  0.0765229  0.0149449 *0.5248162 \n",
            "         。 *0.6071482  0.0069642  0.1370523  0.0707598  0.1780756 \n",
            "      </s> *0.5006309  0.0243729  0.1555425  0.1442633  0.1751905 \n",
            "\n",
            "[2021-11-07 07:21:11,294 INFO]                 Later         in        his       life         he       also       went         by        the       name      Kigen          . \n",
            "         後  0.0064401  0.0111978  0.0102053  0.0613389  0.0198825  0.0015155  0.0378379  0.0284030  0.1335796 *0.4042363  0.1749235  0.1104396 \n",
            "         に  0.0004686  0.0032318  0.0055654  0.0120450  0.0212368  0.0007868  0.1278985  0.0386190  0.1257744 *0.3332547  0.1270655  0.2040535 \n",
            "     <unk>  0.0001650  0.0026424  0.0034688  0.0442357  0.0634963  0.0002959 *0.5423210  0.0093852  0.0203701  0.0706608  0.1710277  0.0719311 \n",
            "         を  0.0002118  0.0007369  0.0010110  0.0025890  0.0876881  0.0004460 *0.4773925  0.0370046  0.0337576  0.0522948  0.1314452  0.1754224 \n",
            "        名乗  0.0006974  0.0061534  0.0069408  0.0096263  0.0191267  0.0002130 *0.5147473  0.0614626  0.0654833  0.0787444  0.0544602  0.1823446 \n",
            "         る  0.0003640  0.0009426  0.0004687  0.0008533  0.0385384  0.0003541  0.0386777  0.0187406  0.0564164  0.0508862  0.0376358 *0.7561222 \n",
            "         。  0.0011730  0.0052918  0.0060743  0.0190817  0.1088133  0.0004410  0.1328227  0.0654216  0.0812930 *0.3243152  0.0937292  0.1615434 \n",
            "      </s>  0.0687793  0.0381534  0.0118412  0.0104158  0.0268046  0.0398230  0.0334875  0.0658322  0.1011416  0.0329748  0.0627943 *0.5079524 \n",
            "\n",
            "[2021-11-07 07:21:11,307 INFO]                Within        the       sect         he         is    referre         to         by        the    honorar      title       Koso          . \n",
            "        宗派  0.0296114  0.0020392  0.0160891  0.0437419  0.1595858  0.0871929  0.0674591  0.0070215  0.0110142 *0.2929185  0.1076219  0.1223015  0.0534030 \n",
            "         内  0.0051260  0.0005662  0.0005532  0.0436261  0.0494278  0.0280382  0.0216704  0.0131377  0.0147906  0.1179959  0.0601770  0.0721164 *0.5727746 \n",
            "         で  0.0046023  0.0022856  0.0027911  0.0302080  0.0582711  0.0456672  0.0454398  0.0088701  0.0144093  0.1597936  0.0337310  0.1497217 *0.4442090 \n",
            "         は  0.0051050  0.0011861  0.0040801  0.0174783  0.0625814  0.0657120  0.0555952  0.0072320  0.0117874 *0.3611861  0.1375652  0.1420183  0.1284729 \n",
            "     <unk>  0.0006306  0.0000376  0.0001293  0.0022153  0.0113780  0.0324537  0.0347219  0.0009367  0.0022078 *0.3999346  0.2298513  0.2771021  0.0084011 \n",
            "         。  0.0002540  0.0001269  0.0001679  0.0141157  0.0107013  0.0359518  0.0348403  0.0143797  0.0106617  0.3477100 *0.4364691  0.0700958  0.0245258 \n",
            "      </s>  0.0081125  0.0392229  0.0222439  0.0198480  0.0591122  0.0189375  0.0251660  0.0478693  0.1044575  0.0448011  0.0608046  0.1620885 *0.3873360 \n",
            "\n",
            "[2021-11-07 07:21:11,311 INFO]               Posthum      named     Bussho      Dento    Kokushi          ,         or    Joyo-Da          . \n",
            "     <unk> *0.2177311  0.1234780  0.2016865  0.1805533  0.1366128  0.0630083  0.0053653  0.0210892  0.0504754 \n",
            "         は  0.0844892  0.0327429  0.2092975  0.2615430 *0.3169394  0.0179471  0.0049107  0.0128850  0.0592453 \n",
            "     <unk>  0.0445533  0.0550532 *0.2770521  0.1902137  0.1393199  0.1429708  0.0079879  0.0547616  0.0880875 \n",
            "        国師  0.0573533  0.0251713  0.1830162  0.2042717 *0.3849238  0.0188687  0.0034146  0.0250782  0.0979021 \n",
            "         、  0.0060878  0.0384934  0.0811611  0.1580859  0.1387830  0.2372941  0.0113789 *0.2436087  0.0851069 \n",
            "     <unk>  0.0154487  0.0135975  0.0492378  0.0721468 *0.5914210  0.0655612  0.0089531  0.1346209  0.0490131 \n",
            "        国師  0.0155773  0.0047423  0.1036738  0.1299097 *0.4781347  0.0125824  0.0029475  0.1774322  0.0750000 \n",
            "         と  0.0116864  0.0263263  0.0654814  0.0699169  0.0492434  0.1234480  0.0095132 *0.4442498  0.2001345 \n",
            "         い  0.0350956  0.0547195  0.0928711  0.0866052  0.0780147  0.1375190  0.0343037  0.1489889 *0.3318824 \n",
            "         う  0.0246501  0.0523452  0.0417539  0.0167056  0.0028939  0.1052134  0.0256506  0.0795822 *0.6512051 \n",
            "         。  0.0518943  0.0572345  0.1079781  0.1249839  0.0463043  0.1306592  0.0409904  0.1393681 *0.3005872 \n",
            "      </s>  0.0247141  0.0380209  0.0748340  0.0610394  0.0548312  0.1412210  0.0594153  0.1143678 *0.4315563 \n",
            "\n",
            "[2021-11-07 07:21:11,315 INFO]                    He         is    general     called      Dogen      Zenji          . \n",
            "        一般  0.0386042  0.1253566  0.0001141  0.0061731  0.0308617 *0.6843648  0.1145255 \n",
            "         に  0.0510457  0.0660667  0.0031833  0.1248847  0.0664226 *0.5152946  0.1731024 \n",
            "         は  0.0013371  0.0177500  0.0000613  0.0016364  0.0374092 *0.9317530  0.0100530 \n",
            "     <unk>  0.0004324  0.0025723  0.0000021  0.0004458  0.0169416 *0.9756939  0.0039119 \n",
            "        禅師  0.0377667  0.0028846  0.0000045  0.0002610  0.0245352 *0.9265735  0.0079744 \n",
            "         と  0.0527787  0.0263549  0.0004936  0.0323575  0.0248980 *0.5442901  0.3188271 \n",
            "         呼  0.0966979  0.0212682  0.0009433  0.0107011  0.0090538 *0.5153190  0.3460168 \n",
            "         ば  0.0661965  0.0292131  0.0004651  0.0198352  0.0156135  0.2820716 *0.5866050 \n",
            "         れ  0.0209980  0.0204679  0.0003877  0.0042368  0.0028182  0.0636584 *0.8874329 \n",
            "         る  0.0337620  0.0456849  0.0003668  0.0148923  0.0012735  0.0109278 *0.8930929 \n",
            "         。  0.3077952  0.1073673  0.0010921  0.0627480  0.0094975  0.0271367 *0.4843633 \n",
            "      </s>  0.0497564  0.1319849  0.0416039  0.0480478  0.0156070  0.0661434 *0.6468567 \n",
            "\n",
            "[2021-11-07 07:21:11,318 INFO]                    He         is    reputed         to       have       been        the        one       that     spread        the    practic         of      tooth    brushin          ,       face    washing          ,      table    manners        and    cleanin         in      Japan          . \n",
            "        日本  0.0009290  0.0054201  0.0020648  0.0011423  0.0009839  0.0012780  0.0005428  0.0005436  0.0005597  0.0006797  0.0023784  0.0130363  0.0236115 *0.6893281  0.0301757  0.0029984  0.0076490  0.0498206  0.0019528  0.1047211  0.0387067  0.0024139  0.0032368  0.0074786  0.0047048  0.0036436 \n",
            "         に  0.0020536  0.0072890  0.0002694  0.0002202  0.0003502  0.0027255  0.0011052  0.0005466  0.0007385  0.0002752  0.0019189  0.0321093  0.0144358 *0.5311818  0.0509430  0.0116929  0.0020793  0.0411020  0.0038691  0.1452918  0.0688375  0.0162267  0.0105046  0.0022604  0.0013965  0.0505770 \n",
            "        おけ  0.0003011  0.0010128  0.0001023  0.0000425  0.0001248  0.0001931  0.0000991  0.0000183  0.0002258  0.0001862  0.0011927  0.0073610  0.0048783 *0.5116081  0.0551865  0.0048060  0.0112523  0.2027314  0.0064965  0.1200678  0.0447754  0.0108665  0.0055779  0.0017705  0.0032007  0.0059222 \n",
            "         る  0.0058290  0.0233927  0.0016877  0.0011263  0.0011053  0.0125547  0.0283668  0.0056108  0.0073268  0.0003493  0.0012436  0.0002933  0.0022406  0.0055633  0.0068631  0.0297511  0.0020500  0.0039226  0.0481742  0.0030067  0.0039559  0.0191473  0.0088895  0.0017627  0.0016382 *0.7741486 \n",
            "     <unk>  0.0000637  0.0001018  0.0000039  0.0000011  0.0000021  0.0000189  0.0000076  0.0000017  0.0000136  0.0000059  0.0001187  0.0034399  0.0015856 *0.6393526  0.0227276  0.0004641  0.0067491  0.1037401  0.0007681  0.1601490  0.0574279  0.0017392  0.0010053  0.0000650  0.0000972  0.0003503 \n",
            "         ・  0.0013124  0.0007378  0.0000744  0.0000151  0.0000277  0.0001915  0.0001480  0.0000692  0.0001394  0.0003507  0.0010378  0.0088763  0.0042723  0.1947854  0.0330334  0.0029054  0.0496027 *0.3385647  0.0063862  0.2173842  0.1214901  0.0098425  0.0056694  0.0006518  0.0008811  0.0015505 \n",
            "     <unk>  0.0000134  0.0000048  0.0000005  0.0000001  0.0000005  0.0000012  0.0000015  0.0000002  0.0000009  0.0000011  0.0000105  0.0001205  0.0001088  0.0479402  0.0068561  0.0002606  0.0167953 *0.6576180  0.0007273  0.2246187  0.0426137  0.0007960  0.0011599  0.0000428  0.0000126  0.0002947 \n",
            "         ・  0.0019564  0.0012010  0.0001577  0.0000368  0.0000972  0.0005848  0.0006243  0.0001884  0.0003039  0.0002808  0.0005722  0.0019247  0.0013223  0.0319701  0.0143221  0.0047129  0.0651451 *0.4468546  0.0250007  0.1832055  0.1307457  0.0396411  0.0258850  0.0034816  0.0037019  0.0160831 \n",
            "     <unk>  0.0000162  0.0000032  0.0000001  0.0000000  0.0000004  0.0000011  0.0000016  0.0000001  0.0000010  0.0000004  0.0000029  0.0000446  0.0000219  0.0065043  0.0013885  0.0001564  0.0194481 *0.5621498  0.0013856  0.2357999  0.1559180  0.0095068  0.0067130  0.0000844  0.0000381  0.0008135 \n",
            "         ・  0.0039631  0.0028594  0.0002853  0.0000716  0.0002657  0.0015889  0.0020964  0.0005736  0.0009640  0.0004930  0.0008893  0.0029979  0.0014074  0.0141167  0.0089120  0.0052947  0.0388129 *0.3074788  0.0310797  0.1286535  0.1903419  0.1287714  0.0642851  0.0056912  0.0087661  0.0493409 \n",
            "     <unk>  0.0000357  0.0000060  0.0000002  0.0000001  0.0000007  0.0000021  0.0000031  0.0000003  0.0000022  0.0000008  0.0000040  0.0000721  0.0000260  0.0047910  0.0011320  0.0001966  0.0166605 *0.4794367  0.0016795  0.2159509  0.2433201  0.0230801  0.0117747  0.0001393  0.0000629  0.0016227 \n",
            "         ・  0.0054608  0.0042112  0.0003113  0.0000817  0.0004081  0.0021673  0.0031856  0.0009914  0.0016801  0.0007951  0.0013215  0.0049525  0.0018864  0.0130772  0.0090449  0.0064283  0.0261701 *0.2631541  0.0287734  0.1092066  0.1910000  0.1725256  0.0709742  0.0050676  0.0083809  0.0687441 \n",
            "     <unk>  0.0000863  0.0000130  0.0000004  0.0000001  0.0000017  0.0000044  0.0000068  0.0000011  0.0000054  0.0000020  0.0000068  0.0001205  0.0000412  0.0051872  0.0013240  0.0003072  0.0160406 *0.4445044  0.0020904  0.2124828  0.2645846  0.0351428  0.0147347  0.0001932  0.0000857  0.0030329 \n",
            "        など  0.0083855  0.0064039  0.0003804  0.0001027  0.0006301  0.0029065  0.0045667  0.0017556  0.0028835  0.0015263  0.0023088  0.0095112  0.0030827  0.0169840  0.0120733  0.0092695  0.0209935 *0.2471815  0.0275691  0.1074273  0.1736171  0.1838546  0.0620905  0.0042211  0.0070138  0.0832606 \n",
            "         を  0.0024041  0.0214501  0.0030971  0.0003900  0.0018047  0.0190179  0.0308535  0.0134376  0.0117632  0.0048931  0.0167214  0.0201863  0.0345978  0.1185567  0.0515284  0.0456790  0.0063667  0.0370563  0.0489166  0.0331303  0.0345761  0.0565144  0.0265517  0.0055079  0.0071633 *0.3478356 \n",
            "     <unk>  0.0006282  0.0002482  0.0000073  0.0000030  0.0000365  0.0000454  0.0001504  0.0001029  0.0004096  0.0011622  0.0028552  0.0297924  0.0102160  0.0683336  0.0142275  0.0031348  0.0053901 *0.6190266  0.0014869  0.1168844  0.1111600  0.0046593  0.0036352  0.0003140  0.0001887  0.0059016 \n",
            "         し  0.0192428  0.0147680  0.0016728  0.0004465  0.0014474  0.0027546  0.0027932  0.0020545  0.0032074  0.0054839  0.0099406  0.0496600  0.0423278 *0.3723121  0.0798237  0.0315620  0.0077684  0.1599241  0.0057253  0.1179304  0.0172844  0.0047741  0.0037148  0.0026195  0.0052294  0.0355324 \n",
            "         た  0.0071267  0.0374486  0.0057573  0.0026934  0.0052087  0.0184313  0.0216681  0.0070688  0.0085553  0.0017973  0.0046123  0.0039304  0.0168456 *0.2542849  0.0495240  0.0482900  0.0152578  0.0531654  0.0401188  0.0751460  0.0291122  0.0189949  0.0117431  0.0214012  0.0297675  0.2120504 \n",
            "     <unk>  0.0436034  0.0314187  0.0125793  0.0006298  0.0011195  0.0123355  0.0097062  0.0052952  0.0104038  0.0155293  0.0050934  0.0464375  0.0210023 *0.3207862  0.0695055  0.0193578  0.0117744  0.0781306  0.0065436  0.1392899  0.0339468  0.0265251  0.0049732  0.0010001  0.0041389  0.0688740 \n",
            "         で *0.2304955  0.1043322  0.0166231  0.0024273  0.0047033  0.0136195  0.0107823  0.0150293  0.0208826  0.0204624  0.0088394  0.0556768  0.0217885  0.0941642  0.0396270  0.0204745  0.0112049  0.0895822  0.0053466  0.0653645  0.0308136  0.0288995  0.0081943  0.0008735  0.0018348  0.0779582 \n",
            "         あ  0.0415078  0.1051315  0.0138274  0.0080491  0.0255855  0.0293564  0.0446858  0.0381601  0.0311400  0.0101345  0.0071786  0.0046584  0.0160390  0.0500857  0.0155633  0.0278204  0.0061625  0.0392171  0.0128328  0.0099407  0.0074570  0.0157216  0.0070914  0.0089573  0.0187112 *0.4049849 \n",
            "         る  0.0221068  0.0376451  0.0048883  0.0028054  0.0038362  0.0092088  0.0212572  0.0063340  0.0120150  0.0006385  0.0008124  0.0001636  0.0017596  0.0032113  0.0041336  0.0241967  0.0010248  0.0034880  0.0086989  0.0011756  0.0007971  0.0047358  0.0032403  0.0010515  0.0009847 *0.8197907 \n",
            "         。  0.0438473  0.0591572  0.0210375  0.0024642  0.0027698  0.0164811  0.0286548  0.0197649  0.0153297  0.0014254  0.0035392  0.0014960  0.0081477  0.0541640  0.0283306  0.0348366  0.0143891  0.0366407  0.0185825  0.0322832  0.0143965  0.0095837  0.0123498  0.0119044  0.0066920 *0.5017321 \n",
            "      </s>  0.0122952  0.0441910  0.0368042  0.0428653  0.0434654  0.0432556  0.0957460  0.0659242  0.0433639  0.0133722  0.0111116  0.0017953  0.0167351  0.0244695  0.0195533  0.0791707  0.0092350  0.0080634  0.0605207  0.0056445  0.0050667  0.0184367  0.0109758  0.0154336  0.0121947 *0.2603105 \n",
            "\n",
            "[2021-11-07 07:21:11,322 INFO]               Another      story        has         it       that         he        was        the      first        one         to      bring    Moso-ch          (       Moso     bamboo          )         to      Japan          . \n",
            "        一説  0.0006156  0.0116315  0.0088564  0.0043063  0.0059527  0.0029716  0.0051943  0.0044072  0.0021633  0.0014273  0.0041059  0.0141239  0.0355155  0.0383263  0.0172945 *0.6754786  0.0067430  0.0789495  0.0753252  0.0066114 \n",
            "         に  0.0014127  0.0424063  0.0377421  0.0169546  0.0138122  0.0372717  0.0332628  0.0171587  0.0168107  0.0751803  0.0389270  0.0531959  0.0286977  0.0222995  0.0282489  0.1583986  0.0561458  0.0450600  0.0410033 *0.2360114 \n",
            "         は  0.0000075  0.0005760  0.0003007  0.0003776  0.0008743  0.0006328  0.0009399  0.0002492  0.0001567  0.0000879  0.0006731  0.0019885  0.0245420  0.0109249  0.0118553 *0.9079925  0.0017059  0.0129714  0.0212912  0.0018526 \n",
            "     <unk>  0.0000075  0.0006027  0.0004457  0.0003800  0.0005457  0.0004904  0.0005861  0.0002858  0.0001787  0.0001469  0.0004534  0.0020230  0.0168475  0.0085590  0.0070730 *0.9284637  0.0017270  0.0181100  0.0115655  0.0015084 \n",
            "         を  0.0000010  0.0002930  0.0003400  0.0002200  0.0001722  0.0007353  0.0000831  0.0000634  0.0000275  0.0001320  0.0011385  0.0336636  0.0097460  0.0047548  0.0062875 *0.9118901  0.0023853  0.0090225  0.0172524  0.0017919 \n",
            "        日本  0.0000000  0.0000060  0.0000052  0.0000043  0.0000054  0.0000174  0.0000024  0.0000027  0.0000013  0.0000035  0.0000915  0.0061885  0.0019861  0.0026763  0.0027096 *0.9740213  0.0004956  0.0081901  0.0035019  0.0000911 \n",
            "         に  0.0000117  0.0007269  0.0028903  0.0026374  0.0020161  0.0046796  0.0029170  0.0009060  0.0001850  0.0008678  0.0099152  0.0647777  0.0298636  0.0176819  0.0152107 *0.7005671  0.0420974  0.0384638  0.0067286  0.0568562 \n",
            "       もたら  0.0000028  0.0005775  0.0005191  0.0004075  0.0006823  0.0018451  0.0002268  0.0002242  0.0001238  0.0004759  0.0059901  0.1949130  0.0381268  0.0175063  0.0170203 *0.6178897  0.0106807  0.0655771  0.0210625  0.0061483 \n",
            "         す  0.0000699  0.0011518  0.0035095  0.0071910  0.0058669  0.0237889  0.0019133  0.0010162  0.0002670  0.0028541  0.0106224  0.0283041  0.0214947  0.0083546  0.0299132  0.0401584  0.0957008  0.0021328  0.0035290 *0.7121615 \n",
            "        最初  0.0000217  0.0106185  0.0015244  0.0012493  0.0020365  0.0139866  0.0055882  0.0052749  0.0021364  0.0065371  0.0060951  0.0139724  0.0203808  0.0088446  0.0131969 *0.8241001  0.0055164  0.0168703  0.0310487  0.0110011 \n",
            "         の  0.0001251  0.0112422  0.0155732  0.0123550  0.0173096  0.0344058  0.0169254  0.0159185  0.0054450  0.0190817  0.0535105  0.1086934  0.0364969  0.0175766  0.0491248  0.1661250  0.0595593  0.0153901  0.0190861 *0.3260560 \n",
            "        もの  0.0000004  0.0058760  0.0001235  0.0000671  0.0000991  0.0024149  0.0000436  0.0000113  0.0000057  0.0000670  0.0002325  0.0032203  0.0006791  0.0002138  0.0011779 *0.9842409  0.0001128  0.0003058  0.0008900  0.0002184 \n",
            "         と  0.0005886  0.0025703  0.0130702  0.0186242  0.0158826  0.0048618  0.0077276  0.0079655  0.0014922  0.0052138  0.0027557  0.0009766  0.0053549  0.0040933  0.0049006  0.0009305  0.0705651  0.0006187  0.0012568 *0.8305510 \n",
            "         さ  0.0030523  0.0248739  0.0421361  0.0292499  0.0298968  0.0321605  0.0329826  0.0703850  0.0235512  0.0325108  0.0203629  0.0225062  0.0109071  0.0113494  0.0084570  0.0022725  0.0561772  0.0131352  0.0232639 *0.5107697 \n",
            "         れ  0.0006569  0.0013924  0.0162975  0.0223825  0.0138182  0.0037494  0.0034707  0.0043289  0.0009082  0.0062777  0.0068226  0.0026058  0.0062503  0.0032030  0.0026846  0.0005992  0.0558794  0.0009364  0.0018476 *0.8458886 \n",
            "         る  0.0014617  0.0032301  0.0321414  0.0351769  0.0286673  0.0054232  0.0097290  0.0069075  0.0010416  0.0047343  0.0038032  0.0016682  0.0055673  0.0066922  0.0038404  0.0008373  0.0695742  0.0006595  0.0032922 *0.7755524 \n",
            "         。  0.0047211  0.1079413  0.0509489  0.0674401  0.0551218  0.0467846  0.0186868  0.0115197  0.0016295  0.0174903  0.0057827  0.0015423  0.0099354  0.0039023  0.0067338  0.0056917  0.0453202  0.0009776  0.0023935 *0.5354362 \n",
            "      </s>  0.0330029  0.0064722  0.0441975  0.0722655  0.0677952  0.0133220  0.0296446  0.0541177  0.0387918  0.0437148  0.0245730  0.0114753  0.0279617  0.0263415  0.0243676  0.0066122  0.1011787  0.0105358  0.0096586 *0.3539713 \n",
            "\n",
            "[2021-11-07 07:21:11,327 INFO]                Though       some     points        are    unclear      about      Dogen         's      birth          ,        all    account      agree       that         he        was       born         in        the       line         of    Udaijin          (    Ministe         of        the      Right          )    Michich    TSUCHIM          (    MINAMOT         no    Michich         or    Michich       KOGA          )          . \n",
            "     <unk>  0.0072363  0.0005040  0.0196526  0.0306698  0.0026361  0.0016686  0.0319033  0.0186197  0.0888258  0.0531755  0.0012488  0.0016065  0.0003881  0.0048743  0.0078221  0.0020048  0.0020475  0.0033661  0.0011526  0.0074198  0.0088274  0.0345720  0.0307279  0.0264055  0.0338061  0.0423255 *0.4081542  0.0283516  0.0043611  0.0019642  0.0080002  0.0002189  0.0001959  0.0012695  0.0000744  0.0002471  0.0168786  0.0196919  0.0471056 \n",
            "         の  0.0050389  0.0015426  0.0356436  0.0651212  0.0088828  0.0017457  0.0439015  0.0179449 *0.1270727  0.0830925  0.0025004  0.0092304  0.0016165  0.0086633  0.0539512  0.0127447  0.0166846  0.0068413  0.0014860  0.0059875  0.0168291  0.0234141  0.0166917  0.0561464  0.0273839  0.0083209  0.0579718  0.0577409  0.0279780  0.0060804  0.0137728  0.0001905  0.0002579  0.0103608  0.0010763  0.0005751  0.0008729  0.0421916  0.1224526 \n",
            "        出生  0.0051002  0.0003410  0.0931473  0.0405661  0.0057749  0.0056202  0.0539631  0.0142070 *0.2208883  0.0542383  0.0015945  0.0028934  0.0012460  0.0083325  0.0351948  0.0073625  0.0123361  0.0098185  0.0029124  0.0425992  0.0097041  0.0246083  0.0263789  0.0608467  0.0130201  0.0081669  0.1109793  0.0354475  0.0120852  0.0043711  0.0113423  0.0002743  0.0002083  0.0018035  0.0001533  0.0001494  0.0045469  0.0164979  0.0412795 \n",
            "         に  0.0038653  0.0046884  0.1762781  0.1081175  0.0732187  0.0203369  0.0123022  0.0196072  0.0318699  0.1443097  0.0006556  0.0082443  0.0016074  0.0058718  0.0270564  0.0024398  0.0025013  0.0056396  0.0023058  0.0044813  0.0043428  0.0031455  0.0014590  0.0080471  0.0030244  0.0008450  0.0007915  0.0248446  0.0014395  0.0020258  0.0020272  0.0000525  0.0000624  0.0007417  0.0000667  0.0000654  0.0000169  0.0501404 *0.2414645 \n",
            "         つ  0.0011374  0.0001727  0.0014007  0.0321505  0.0179783  0.0010630  0.0091354  0.0111224  0.0012468  0.1032740  0.0006361  0.0013158  0.0001508  0.0031985  0.0109852  0.0020683  0.0010524  0.0085712  0.0057933  0.0062598  0.0226219  0.0233975  0.0038060  0.0020966  0.0086001  0.0242267  0.0073304  0.0505746  0.0001212  0.0003631  0.0008444  0.0002575  0.0004472  0.0001511  0.0000075  0.0000544  0.0003608  0.1376637 *0.4983627 \n",
            "         い  0.0038044  0.0010925  0.0006316  0.0307192  0.0106014  0.0005674  0.0002370  0.0013641  0.0002673  0.0782378  0.0040919  0.0046297  0.0035135  0.0091609  0.0028296  0.0007809  0.0003114  0.0028465  0.0056038  0.0030356  0.0620078  0.0295500  0.0115413  0.0017330  0.0043280  0.0116469  0.0010533  0.0683576  0.0004921  0.0017755  0.0068886  0.0001732  0.0002934  0.0001450  0.0002607  0.0000901  0.0000914  0.1749142 *0.4603312 \n",
            "         て  0.0070524  0.0017191  0.0010864  0.0359563  0.0047971  0.0005818  0.0001789  0.0009914  0.0003850  0.1780269  0.0057493  0.0050139  0.0032603  0.0120186  0.0068596  0.0011511  0.0004269  0.0021394  0.0034062  0.0027769  0.0196260  0.0121136  0.0073548  0.0016427  0.0021131  0.0067204  0.0012536  0.0779335  0.0007461  0.0016147  0.0049404  0.0013101  0.0017135  0.0003335  0.0012968  0.0012223  0.0014312  0.1613940 *0.4216624 \n",
            "         は  0.0035272  0.0005995  0.0028112  0.0428450  0.0096644  0.0012362  0.0317792  0.0126160  0.0061460  0.0526106  0.0043845  0.0024546  0.0003242  0.0046611  0.0041592  0.0020381  0.0012856  0.0030806  0.0015060  0.0012282  0.0219608  0.0555973  0.0266511  0.0040067  0.0249271  0.1394492 *0.3767103  0.0260812  0.0000988  0.0001950  0.0021099  0.0000802  0.0001412  0.0000343  0.0000107  0.0000019  0.0001028  0.0280800  0.1048042 \n",
            "         、  0.0022593  0.0001945  0.0016255  0.0247683  0.0048275  0.0010116  0.0221091  0.0094248  0.0120309  0.0856278  0.0010390  0.0007346  0.0000968  0.0018216  0.0038614  0.0009786  0.0008002  0.0032988  0.0009648  0.0013118  0.0072558  0.0206238  0.0125646  0.0039041  0.0157861  0.0651277 *0.4146425  0.0373529  0.0001323  0.0002399  0.0029083  0.0001190  0.0001345  0.0000712  0.0000230  0.0000186  0.0022771  0.0541084  0.1839232 \n",
            "     <unk>  0.0017945  0.0001132  0.0011206  0.0092759  0.0011635  0.0002619  0.0033381  0.0023095  0.0045524  0.0244912  0.0007193  0.0003877  0.0000754  0.0012322  0.0012313  0.0004486  0.0003323  0.0016526  0.0008616  0.0031173  0.0091475  0.0457567  0.0291215  0.0057604  0.0221800  0.0812641 *0.6441137  0.0281283  0.0004481  0.0006893  0.0067872  0.0001615  0.0001639  0.0001338  0.0000265  0.0000164  0.0045570  0.0202018  0.0428630 \n",
            "     <unk>  0.0021366  0.0004823  0.0033010  0.0100187  0.0020744  0.0002904  0.0158867  0.0024959  0.0071885  0.0138785  0.0006248  0.0024168  0.0003317  0.0027671  0.0056317  0.0011729  0.0005911  0.0027077  0.0022099  0.0112030  0.0215346  0.1072160  0.0554864  0.1260472  0.1210895  0.0215901 *0.1801335  0.0455571  0.1005875  0.0286150  0.0280898  0.0003709  0.0004573  0.0243330  0.0012428  0.0006440  0.0049717  0.0205007  0.0241235 \n",
            "        通親  0.0000696  0.0000161  0.0002360  0.0008148  0.0001411  0.0000652  0.0002854  0.0000955  0.0008069  0.0016249  0.0000469  0.0003164  0.0000538  0.0006025  0.0010320  0.0002796  0.0002476  0.0034346  0.0028962  0.0187063  0.0131391  0.0518990  0.0138905  0.1451049  0.0412387  0.0069632  0.1047690  0.0170384 *0.3824728  0.1083646  0.0287628  0.0002131  0.0001323  0.0293504  0.0002254  0.0008625  0.0122786  0.0061613  0.0053619 \n",
            "         （  0.0000743  0.0000181  0.0002781  0.0015025  0.0004240  0.0002281  0.0000029  0.0000782  0.0005725  0.0069368  0.0001500  0.0013819  0.0002753  0.0022392  0.0012067  0.0005753  0.0011244  0.0120177  0.0244991  0.1229942  0.0450415  0.0229750  0.0105729  0.0160682  0.0036744  0.0013532  0.0028584  0.0507141  0.1082292  0.1570154 *0.2120097  0.0004957  0.0004680  0.0157133  0.0009534  0.0018328  0.0049797  0.0632017  0.1052641 \n",
            "         源  0.0001676  0.0000056  0.0000923  0.0000915  0.0000230  0.0000015  0.0000102  0.0000109  0.0000217  0.0002755  0.0001161  0.0001229  0.0000378  0.0001767  0.0001175  0.0000331  0.0000143  0.0001497  0.0002485  0.0092853  0.0012576  0.0047911  0.0068000  0.0031808  0.0109190  0.0169528 *0.7296118  0.0043145  0.0002279  0.0002973  0.0215327  0.0001980  0.0001364  0.0002278  0.0042086  0.0005800  0.1798740  0.0019784  0.0019095 \n",
            "        通親  0.0006915  0.0000179  0.0003698  0.0013301  0.0004047  0.0001292  0.0007475  0.0003238  0.0002029  0.0121397  0.0001040  0.0016805  0.0002457  0.0020641  0.0129464  0.0006542  0.0001721  0.0054928  0.0036844  0.0087701  0.0164654  0.1010089  0.0298913  0.0359128  0.0222585  0.0069254  0.0206020 *0.1972833  0.0147689  0.0227786  0.0270800  0.0029570  0.0049359  0.1211917  0.0164580  0.0487715  0.0342519  0.1000047  0.1242827 \n",
            "         ・  0.0007633  0.0000963  0.0059180  0.0086966  0.0031686  0.0020235  0.0000970  0.0003017  0.0004777  0.0132507  0.0001875  0.0071557  0.0011104  0.0062246  0.0046759  0.0003194  0.0003044  0.0065231  0.0068593  0.0079296  0.0191659  0.0296123  0.0079078  0.0571089  0.0106843  0.0042266  0.0160960  0.0676351  0.0686021  0.1328966 *0.1689746  0.0020581  0.0016448  0.0347136  0.0040918  0.0322979  0.0612170  0.0985253  0.1064583 \n",
            "        久我  0.0000501  0.0000017  0.0000060  0.0000552  0.0000353  0.0000011  0.0000000  0.0000001  0.0000002  0.0009982  0.0000096  0.0000142  0.0000062  0.0000813  0.0000106  0.0000002  0.0000001  0.0000392  0.0000889  0.0000734  0.0000953  0.0002694  0.0001252  0.0005276  0.0003620  0.0002045  0.0020462  0.0040732  0.0000680  0.0011582  0.0128653  0.0001416  0.0000418  0.0000272  0.0005164  0.0060983 *0.9510176  0.0110671  0.0078234 \n",
            "        通親  0.0006986  0.0000228  0.0005778  0.0032718  0.0009857  0.0001572  0.0000261  0.0000251  0.0000050  0.0110227  0.0000097  0.0005627  0.0000841  0.0017754  0.0010719  0.0000324  0.0000078  0.0018044  0.0014610  0.0020240  0.0050993  0.0154255  0.0026849  0.0084915  0.0015619  0.0001871  0.0002633  0.0783967  0.0085979  0.0689181  0.0465947  0.0000490  0.0000538  0.0078732  0.0018758  0.1466049  0.1069419  0.2019839 *0.2727705 \n",
            "         ）  0.0033794  0.0005144  0.0130299  0.0321193  0.0096348  0.0070890  0.0000221  0.0004882  0.0001910  0.0423088  0.0002273  0.0054778  0.0042340  0.0127311  0.0037591  0.0002295  0.0002527  0.0044529  0.0037105  0.0030477  0.0126290  0.0033315  0.0010805  0.0024159  0.0003026  0.0001106  0.0000407  0.0442327  0.0049082  0.0190212  0.0744187  0.0001130  0.0001341  0.0018864  0.0031452  0.0175047  0.0017081  0.1962470 *0.4698705 \n",
            "         の  0.0000253  0.0000023  0.0000110  0.0004600  0.0000811  0.0000095  0.0000003  0.0000047  0.0000420  0.0026720  0.0000249  0.0001656  0.0000259  0.0006932  0.0003374  0.0001748  0.0002527  0.0082865  0.0216170  0.1025337  0.0314129  0.0282219  0.0139101  0.0124911  0.0020384  0.0009757  0.0020006  0.0786572  0.0764075  0.1517024 *0.2979184  0.0002943  0.0002701  0.0071346  0.0002881  0.0014582  0.0143351  0.0718651  0.0711984 \n",
            "        系統  0.0001250  0.0000080  0.0003350  0.0012614  0.0008146  0.0000604  0.0000044  0.0000212  0.0000952  0.0017770  0.0000463  0.0004436  0.0001290  0.0020574  0.0009481  0.0005641  0.0009442  0.0157826  0.0185888 *0.3564319  0.0384581  0.0574041  0.0277504  0.1077814  0.0141777  0.0017938  0.0116694  0.0434975  0.0187768  0.0315433  0.1722728  0.0000845  0.0000731  0.0021885  0.0001797  0.0006297  0.0122154  0.0297397  0.0293257 \n",
            "         で  0.0016045  0.0008476  0.0012266  0.0165809  0.0050446  0.0001412  0.0001416  0.0001283  0.0001529  0.0155181  0.0010039  0.0055436  0.0006029  0.0050126  0.0072004  0.0050430  0.0030651  0.0290313  0.0164737  0.0312105  0.0478893  0.0366536  0.0091663  0.0151021  0.0127617  0.0096014  0.0178799  0.1042705  0.0237639  0.0207904  0.0269351  0.0004900  0.0003999  0.0036045  0.0002889  0.0007980  0.0037402  0.1738656 *0.3464254 \n",
            "       生まれ  0.0024701  0.0002788  0.0142997  0.0132860  0.0136888  0.0003161  0.0011371  0.0009408  0.0003557  0.0114645  0.0026266  0.0036993  0.0007226  0.0037907  0.0311604  0.0223149  0.0289185  0.0252203  0.0057952  0.0571931  0.0512416  0.0763334  0.0086624  0.1489983 *0.1574265  0.0430511  0.0899912  0.0257714  0.0043774  0.0035605  0.0048173  0.0001090  0.0001823  0.0004930  0.0000146  0.0001158  0.0035235  0.0343390  0.1073126 \n",
            "         た  0.0003756  0.0000365  0.0001713  0.0062442  0.0015377  0.0000465  0.0000851  0.0001599  0.0000059  0.0311420  0.0000617  0.0006102  0.0001520  0.0021445  0.0095088  0.0005270  0.0001369  0.0026446  0.0011302  0.0025458  0.0053432  0.0017777  0.0001260  0.0001664  0.0001507  0.0002577  0.0000400  0.0302557  0.0005217  0.0005667  0.0006448  0.0000778  0.0000694  0.0002225  0.0000507  0.0049502  0.0042663  0.1781425 *0.7131037 \n",
            "         と  0.0007103  0.0001123  0.0045537  0.0157398  0.0045591  0.0003894  0.0002538  0.0004287  0.0008104  0.0572763  0.0002338  0.0027183  0.0007200  0.0026595  0.0434127  0.0081399  0.0079357  0.0107927  0.0015075  0.0018673  0.0074845  0.0055593  0.0002424  0.0017477  0.0009484  0.0008179  0.0004978  0.0340190  0.0024860  0.0055626  0.0007958  0.0000195  0.0000263  0.0003988  0.0000045  0.0000889  0.0003156  0.1277812 *0.6463829 \n",
            "        する  0.0029439  0.0002201  0.0011517  0.0130285  0.0038087  0.0005563  0.0002380  0.0013136  0.0035174  0.1339296  0.0233030  0.0129847  0.0072114  0.0074585  0.0119826  0.0034346  0.0042649  0.0099834  0.0052270  0.0034801  0.0512544  0.0169001  0.0077485  0.0036530  0.0064871  0.0163000  0.0061580  0.0484751  0.0005234  0.0009885  0.0027522  0.0007999  0.0008725  0.0000800  0.0000698  0.0000499  0.0012087  0.1343867 *0.4512541 \n",
            "         説  0.0026334  0.0002555  0.0037474  0.0185123  0.0024866  0.0011413  0.0001798  0.0007152  0.0033254  0.0578107  0.0074650  0.0497690  0.0292346  0.0112685  0.0165677  0.0072691  0.0070164  0.0223421  0.0041724  0.0063856  0.0678147  0.0559567  0.0128278  0.0107702  0.0082364  0.0272116  0.0234036  0.0565067  0.0045820  0.0044184  0.0023798  0.0000487  0.0000485  0.0008028  0.0000181  0.0000375  0.0003078  0.0829716 *0.3893590 \n",
            "         が  0.0152094  0.0051080  0.0019128  0.0549917  0.0390580  0.0017368  0.0000206  0.0001126  0.0000490  0.0537423  0.0037991  0.0083197  0.0088220  0.0079859  0.0006089  0.0002107  0.0000617  0.0050588  0.0062578  0.0014938  0.0181804  0.0125132  0.0040646  0.0004392  0.0013438  0.0103399  0.0025418  0.0400782  0.0001648  0.0020033  0.0068168  0.0000380  0.0000272  0.0000182  0.0000415  0.0000080  0.0001425  0.1865447 *0.5001342 \n",
            "         あ  0.0793317  0.0133556  0.0133275  0.1102674  0.0142838  0.0028330  0.0077319  0.0005349  0.0006929  0.1140502  0.0098561  0.0082038  0.0052665  0.0080412  0.0016259  0.0000836  0.0000132  0.0004678  0.0002436  0.0000571  0.0012030  0.0107444  0.0140170  0.0040962  0.0074921  0.0582694 *0.3627770  0.0249155  0.0000848  0.0002515  0.0017727  0.0001076  0.0000561  0.0000084  0.0000734  0.0000052  0.0009506  0.0340185  0.0888884 \n",
            "         る  0.0079667  0.0014425  0.0017599  0.0426694  0.0058900  0.0005789  0.0000504  0.0002365  0.0000428  0.1102122  0.0006199  0.0024895  0.0019144  0.0090733  0.0018551  0.0000968  0.0000201  0.0017909  0.0023243  0.0006342  0.0097581  0.0076885  0.0019739  0.0003044  0.0004862  0.0026167  0.0007000  0.0628560  0.0002718  0.0016069  0.0037944  0.0000880  0.0000995  0.0000757  0.0001301  0.0001570  0.0002862  0.1937159 *0.5217231 \n",
            "         。  0.0233786  0.0058490  0.0051088  0.0650107  0.0443489  0.0051196  0.0008378  0.0013815  0.0002218  0.0846063  0.0020473  0.0098395  0.0109281  0.0153851  0.0046877  0.0004655  0.0001075  0.0058499  0.0037652  0.0008040  0.0113821  0.0119488  0.0018031  0.0005819  0.0014970  0.0093316  0.0029248  0.0422155  0.0002308  0.0016682  0.0023182  0.0000553  0.0000493  0.0000793  0.0001009  0.0001375  0.0007092  0.1593890 *0.4638349 \n",
            "      </s>  0.0195114  0.0189060  0.0080917  0.0737333  0.0479411  0.0126399  0.0037919  0.0083373  0.0008687  0.1617528  0.0274191  0.0102241  0.0116038  0.0269867  0.0076601  0.0020574  0.0010425  0.0106879  0.0138392  0.0049687  0.0256914  0.0157133  0.0091388  0.0032249  0.0088006  0.0410724  0.0099838  0.0439706  0.0006294  0.0031260  0.0085948  0.0043155  0.0063201  0.0004227  0.0016213  0.0018274  0.0041738  0.1215861 *0.2177235 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZ6AiX9W8_OE"
      },
      "source": [
        "# アテンションの利用"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UuX1SPL8-bl"
      },
      "source": [
        "import onmt\n",
        "import onmt.inputters\n",
        "import onmt.translate\n",
        "import onmt.model_builder\n",
        "from collections import namedtuple\n",
        "\n",
        "# Load the model.\n",
        "Opt = namedtuple('Opt', ['model', 'data_type', 'reuse_copy_attn', \"gpu\"])\n",
        "\n",
        "opt = Opt(\"/Users/esalesky/projects/models/lstm_clean_acc_51.81_ppl_15.58_e13.pt\", \"text\",False, 0)\n"
      ],
      "execution_count": 23,
      "outputs": []
    }
  ]
}